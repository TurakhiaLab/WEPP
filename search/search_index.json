{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"WEPP: Wastewater-Based Epidemiology using Phylogenetic Placements","text":""},{"location":"index.html#wepp-video-tutorial","title":"WEPP Video Tutorial","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>WEPP (Wastewater-Based Epidemiology using Phylogenetic Placements) is a pathogen-agnostic pipeline that significantly enhances the resolution and capabilities of wastewater surveillance. It analyzes the wastewater sequencing reads by considering the comprehensive phylogeny of the pathogen, specifically, mutation-annotated trees (MATs) that include all globally available clinical sequences and their inferred ancestral nodes, to identify a subset of haplotypes most likely present in the sample. In addition, WEPP reports the abundance of each haplotype and its corresponding lineage, provides parsimonious mappings of individual reads to haplotypes, and flags Unaccounted Alleles \u2014 those observed in the sample but unexplained by selected haplotypes, which may signal the presence of novel circulating variants (Figure 1A). </p> <p>WEPP includes an interactive visualization dashboard that allows users to visualize detected haplotypes and haplotype clusters within the context of the global phylogenetic tree and investigate haplotype and lineage abundances (Figure 1B(i)). It also allows a detailed read-level analysis by selecting a haplotype to view its characteristic mutations alongside those observed in the mapped reads (Figure 1B(ii)). Additional information about individual reads or haplotypes can be accessed by clicking on their corresponding objects, as shown in Figures 1B(iii) and 1B(iv), respectively. </p> <p>WEPP performs parsimonious placement of reads on the MAT and selects a subset of haplotypes along with their nearest neighbors to form a pool of candidate haplotypes. This pool is passed to a deconvolution algorithm to estimate their relative abundances. WEPP only retains haplotypes above an abundance threshold and iteratively refines this set by adding neighbors of the retained set, followed by deconvolution. This process continues until it reaches convergence or a maximum iteration count (Figure 1C). WEPP also uses an outlier detection algorithm on the deconvolution residue to generate a list of Unaccounted Alleles.</p> Figure 1: Overview of the WEPP pipeline. (A) WEPP input and output. (B) Features of the interactive Dashboard: (i) Phylogenetic view of WEPP-inferred haplotypes with their proportions, associated lineages, and uncertain haplotypes.Unaccounted Alleles and their possible haplotype sources are shown in a separate panel; (ii) Read analysis panel highlighting accounted and Unaccounted Alleles contained in reads mapped to a selected haplotype; (iii) Read information panel displaying all possible haplotypes and Unaccounted Alleles for a selected read; (iv) Haplotype information panel listing the possible Unaccounted Alleles associated with the selected haplotype. (C) Key stages of WEPP\u2019s phylogenetic algorithm for haplotype detection and abundance estimation."},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#haplotype-proportions","title":"Haplotype Proportions","text":"<p>WEPP's Phylogenetic Placement of reads enables accurate estimation of haplotype proportions from wastewater samples. These estimates can be interactively explored from the phylogenetic view of the dashboard (Figure 1B(i)), which displays each haplotype\u2019s abundance, associated lineage, and phylogenetic uncertainty via Uncertain Haplotypes - neighboring haplotypes that cannot be confidently disambiguated.</p>"},{"location":"index.html#lineage-proportions","title":"Lineage Proportions","text":"<p>WEPP infers lineage proportions by aggregating haplotype abundances within each lineage, accounting for intra-lineage diversity to produce more accurate and robust estimates.</p>"},{"location":"index.html#unaccounted-alleles","title":"Unaccounted Alleles","text":"<p>WEPP reports a list of Unaccounted Alleles - alleles observed in wastewater that are not explained by the selected haplotypes, along with the inferred haplotype(s) they are most likely associated with (Figure 1B(i)). These Unaccounted Alleles can serve as early indicators of novel variants and often resemble the 'cryptic' mutations described in previous studies.</p>"},{"location":"index.html#read-level-analysis","title":"Read-Level Analysis","text":"<p>WEPP supports detailed analysis of sequencing reads in the context of selected haplotypes (Figure 1B(ii)). It also facilitates interpretation of Unaccounted Alleles by examining their presence in reads relative to the haplotypes they are mapped to. Additional information about individual reads or haplotypes can be accessed by selecting them within the interactive panel (Figure 1B(iii) and Figure 1B(iv)).</p>"},{"location":"cite.html","title":"Cite WEPP","text":"<p>If you use WEPP in your research or publications, please cite the following paper: Pranav Gangwar, Pratik Katte, Manu Bhatt, Yatish Turakhia, \"WEPP: Phylogenetic Placement Achieves Near-Haplotype Resolution in Wastewater-Based Epidemiology\", medRxiv 2025.06.09.25329287; doi: 10.1101/2025.06.09.25329287</p>"},{"location":"contribution.html","title":"Contributions","text":"<p>We welcome contributions from the community to enhance the capabilities of WEPP. If you encounter any issues or have suggestions for improvement, please open an issue on WEPP GitHub page. For general inquiries and support, reach out to our team.</p>"},{"location":"install.html","title":"Install","text":"<p>WEPP offers multiple installation methods. Using a Docker is recommended to prevent any conflict with existing packages.</p> <ol> <li>Docker image from DockerHub</li> <li>Dockerfile </li> <li>Shell Commands </li> </ol> <p>Note</p> <p>\u26a0\ufe0fThe Docker image is currently built for the <code>linux/amd64</code> platform. While it can run on <code>arm64</code> systems (e.g., Apple Silicon or Linux aarch64) via emulation, this may lead to reduced performance.</p>"},{"location":"install.html#option-1-install-via-dockerhub","title":"Option-1: Install via DockerHub","text":"<p>The Docker image includes all dependencies required to run WEPP.</p> <p>Step 1: Get the image from DockerHub  <pre><code>docker pull pranavgangwar/wepp:latest\n</code></pre> Step 2: Start and run Docker container. The command below will take you inside the docker container with WEPP already installed. <pre><code># -p &lt;host_port&gt;:&lt;container_port&gt; \u2192 Maps container port to a port on your host (Accessing Dashboard, NOT needed otherwise)\n# Replace &lt;host_port&gt; with your desired local port (e.g., 80 or 8080)\n# Use this command if your datasets can be downloaded from the Web\ndocker run -it -p 80:80 pranavgangwar/wepp:latest\n\n# Use this command if your datasets are present in your current directory\ndocker run -it -p 80:80 -v \"$PWD\":/WEPP -w /WEPP pranavgangwar/wepp:latest\n</code></pre> Step 3: Confirm proper working by running the following command. This should print WEPP's help menu. <pre><code>snakemake test --cores 1 --use-conda\n</code></pre></p> <p>All set to try the examples.</p>"},{"location":"install.html#option-2-install-via-dockerfile","title":"Option-2: Install via Dockerfile","text":"<p>The Dockerfile contains all dependencies required to run WEPP.</p> <p>Step 1: Clone the repository <pre><code>git clone --recurse-submodules https://github.com/TurakhiaLab/WEPP.git \ncd WEPP\n</code></pre> Step 2: Build a Docker Image <pre><code>cd docker\ndocker build -t wepp . \ncd ..\n</code></pre> Step 3: Start and run Docker container. The command below will take you inside the docker container with the view of the current directory. <pre><code># -p &lt;host_port&gt;:&lt;container_port&gt; \u2192 Maps container port to a port on your host (Accessing Dashboard, NOT needed otherwise)\n# Replace &lt;host_port&gt; with your desired local port (e.g., 80 or 8080)\ndocker run -it -p 80:80 -v \"$PWD\":/workspace -w /workspace wepp\n</code></pre></p> <p>All set to try the examples.</p>"},{"location":"install.html#option-3-install-via-shell-commands-requires-sudo-access","title":"Option-3: Install via Shell Commands (requires sudo access)","text":"<p>Users without sudo access are advised to install WEPP via Docker Image.</p> <p>Step 1: Clone the repository <pre><code>git clone --recurse-submodules https://github.com/TurakhiaLab/WEPP.git\ncd WEPP\n</code></pre> Step 2: Install dependencies (might require sudo access) WEPP depends on the following common system libraries, which are typically pre-installed on most development environments: <pre><code>- wget\n- curl\n- pip\n- build-essential \n- python3-pandas\n- pkg-config\n- zip\n- cmake \n- libtbb-dev\n- libprotobuf-dev\n- protobuf-compiler\n- snakemake\n- conda\n- nodejs(v18+)\n- nginx\n</code></pre></p> <p>For Ubuntu users with sudo access, if any of the required libraries are missing, you can install them with: <pre><code>sudo apt-get update\nsudo apt-get install -y wget pip curl python3-pip build-essential python3-pandas pkg-config zip cmake libtbb-dev libprotobuf-dev protobuf-compiler snakemake nginx\n</code></pre></p> <p>Note: WEPP expects the <code>python</code> command to be available. If your system only provides python3, you can optionally set up a symlink: <pre><code>update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n</code></pre></p> <p>If you do not have Node.js v18 or higher installed, follow these steps to install Node.js v22: <pre><code># Update and install prerequisites\napt-get install -y curl gnupg ca-certificates\n\n# Add NodeSource Node.js 22 repo\ncurl -fsSL https://deb.nodesource.com/setup_22.x | bash -\n\n# Install Node.js 22\napt-get install -y nodejs\n</code></pre></p> <pre><code># Install Yarn package manager globally\nnpm install -g yarn\n# Install TaxoniumTools Python package\npip install taxoniumtools\n</code></pre> <p>If your system doesn't have Conda, you can install it with: <pre><code>wget -O Miniforge3.sh \"https://github.com/conda-forge/miniforge/releases/download/24.11.3-2/Miniforge3-24.11.3-2-Linux-x86_64.sh\"\nbash Miniforge3.sh -b -p \"${HOME}/conda\"\n\nsource \"${HOME}/conda/etc/profile.d/conda.sh\"\nsource \"${HOME}/conda/etc/profile.d/mamba.sh\"\n</code></pre></p> <p>All set to try the examples.</p>"},{"location":"quickstart.html","title":"Quick Start","text":"<p>The following steps will download real wastewater datasets and analyze them using WEPP.</p>"},{"location":"quickstart.html#example-1-rsv-a-dataset-runs-quickly-under-10-minutes-on-32-cores","title":"Example - 1: RSV-A Dataset (Runs Quickly: Under 10 minutes on 32 cores)","text":"<p>Step 1: Download the RSV-A test dataset <pre><code>mkdir -p data/RSVA_real\ncd data/RSVA_real\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR147/011/ERR14763711/ERR14763711_*.fastq.gz https://hgdownload.gi.ucsc.edu/hubs/GCF/002/815/475/GCF_002815475.1/UShER_RSV-A/2025/04/25/rsvA.2025-04-25.pb.gz https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/815/475/GCF_002815475.1_ASM281547v1/GCF_002815475.1_ASM281547v1_genomic.fna.gz\ngunzip GCF_002815475.1_ASM281547v1_genomic.fna.gz \nmv ERR14763711_1.fastq.gz ERR14763711_R1.fastq.gz\nmv ERR14763711_2.fastq.gz ERR14763711_R2.fastq.gz\ncd ../../\n</code></pre> This will save the datasets on a separate data/RSVA_real folder within the repository.</p> <p>Step 2:  Run the pipeline <pre><code>snakemake --config DIR=RSVA_real FILE_PREFIX=test_run PRIMER_BED=RSVA_all_primers_best_hits.bed TREE=rsvA.2025-04-25.pb.gz REF=GCF_002815475.1_ASM281547v1_genomic.fna CLADE_LIST=annotation_1 CLADE_IDX=0 DASHBOARD_ENABLED=True --cores 32 --use-conda\n</code></pre></p> <p>Step 3:  Analyze Results</p> <p>All results generated by WEPP are available in the <code>results/RSVA_real</code> directory. These include haplotype and lineage abundances, associated uncertain haplotypes, and the potential haplotypes corresponding to each detected unaccounted allele.</p> <p>Note</p> <p>\u26a0\ufe0f Make sure port forwarding is enabled when accessing services on external servers.</p>"},{"location":"quickstart.html#example-2-sars-cov-2-dataset-longer-runtime-20-minutes-on-32-cores","title":"Example - 2:  SARS-CoV-2 Dataset (Longer Runtime: ~20 minutes on 32 cores)","text":"<p>Step 1: Download the SARS-CoV-2 test dataset <pre><code>mkdir -p data/SARS_COV_2_real\ncd data/SARS_COV_2_real\nwget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR185/041/SRR18541041/SRR18541041_1.fastq.gz ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR185/041/SRR18541041/SRR18541041_2.fastq.gz https://hgdownload.gi.ucsc.edu/goldenPath/wuhCor1/UShER_SARS-CoV-2/2021/12/05/public-2021-12-05.all.masked.pb.gz\nmv SRR18541041_1.fastq.gz SRR18541041_R1.fastq.gz\nmv SRR18541041_2.fastq.gz SRR18541041_R2.fastq.gz\ncp ../../NC_045512v2.fa .\ncd ../../\n</code></pre> This will save the datasets on a separate data/SARS_COV_2_real folder within the repository.</p> <p>Step 2:  Run the pipeline <pre><code>snakemake --config DIR=SARS_COV_2_real FILE_PREFIX=test_run PRIMER_BED=snap_primers.bed TREE=public-2021-12-05.all.masked.pb.gz REF=NC_045512v2.fa DASHBOARD_ENABLED=True --cores 32 --use-conda\n</code></pre></p> <p>Step 3:  Analyze Results</p> <p>All results generated by WEPP are available in the <code>results/SARS_COV_2_real</code> directory. These include haplotype and lineage abundances, associated uncertain haplotypes, and the potential haplotypes corresponding to each detected unaccounted allele.</p> <p>Note</p> <p>\u26a0\ufe0f Make sure port forwarding is enabled when accessing services on external servers.</p>"},{"location":"usage.html","title":"User Guide","text":""},{"location":"usage.html#organizing-data","title":"Organizing Data","text":"<p>We assume that all wastewater samples are organized in the <code>data</code> directory, each within its own subdirectory given by <code>DIR</code> argument (see Run Command). For each sample, WEPP generates intermediate and output files in corresponding subdirectories under <code>intermediate</code> and <code>result</code>, respectively. </p> <p>Each created <code>DIR</code> inside <code>data</code> is expected to contain the following files:</p> <ol> <li>Sequencing Reads: Ending with <code>*_R{1/2}.fastq.gz</code> for paired-ended reads and <code>*.fastq.gz</code> for single-ended.</li> <li>Reference Genome in FASTA format</li> <li>Mutation-Annotated Tree (MAT)</li> <li>[OPTIONAL] Genome Masking File: <code>mask.bed</code>, whose third column specifies sites to be excluded from analysis.</li> <li>[OPTIONAL] Taxonium <code>.jsonl</code> file to be used for visualizing results in the WEPP dashboard. </li> </ol> <p>Visualization of WEPP's workflow directories <pre><code>\ud83d\udcc1 WEPP\n\u2514\u2500\u2500\u2500\ud83d\udcc1data                                   # [User Created] Contains data to analyze \n    \u251c\u2500\u2500\u2500\ud83d\udcc1SARS_COV_2_real                    # SARS-CoV-2 run wastewater samples - 1\n         \u251c\u2500\u2500\u2500sars_cov_2_reads_R1.fastq.gz    # Paired-ended reads\n         \u251c\u2500\u2500\u2500sars_cov_2_reads_R2.fastq.gz\n         \u251c\u2500\u2500\u2500sars_cov_2_reference.fa \n         \u251c\u2500\u2500\u2500mask.bed                        # OPTIONAL \n         \u251c\u2500\u2500\u2500sars_cov_2_taxonium.jsonl.gz    # OPTIONAL \n         \u2514\u2500\u2500\u2500sars_cov_2_mat.pb.gz\n\n\u2514\u2500\u2500\u2500\ud83d\udcc1intermediate                           # [WEPP Generated] Contains intermediate stage files \n    \u251c\u2500\u2500\u2500\ud83d\udcc1SARS_COV_2_real                \n         \u251c\u2500\u2500\u2500file_1\n         \u2514\u2500\u2500\u2500file_2\n\n\u2514\u2500\u2500\u2500\ud83d\udcc1results                                # [WEPP Generated] Contains final WEPP results\n    \u251c\u2500\u2500\u2500\ud83d\udcc1SARS_COV_2_real                \n         \u251c\u2500\u2500\u2500file_1\n         \u2514\u2500\u2500\u2500file_2\n</code></pre></p>"},{"location":"usage.html#wepp-arguments","title":"WEPP Arguments","text":"<p>The WEPP Snakemake pipeline requires the following arguments, which can be provided either via the configuration file (<code>config/config.yaml</code>) or passed directly on the command line using the <code>--config</code> argument. The command line arguments take precedence over the config file.</p> <ol> <li><code>DIR</code> - Folder name containing the wastewater reads.</li> <li><code>FILE_PREFIX</code> - File Prefix for all intermediate files. </li> <li><code>REF</code> - Reference Genome in fasta.</li> <li><code>TREE</code> - Mutation-Annotated Tree.</li> <li><code>SEQUENCING_TYPE</code> - Sequencing read type (s:Illumina single-ended, d:Illumina double-ended, or n:ONT long reads).</li> <li><code>PRIMER_BED</code> - BED file for primers from the <code>primers</code> folder.</li> <li><code>MIN_AF</code> - Alleles with an allele frequency below this threshold in the reads will be masked (Illumina: 0.5%, Ion Torrent: 1.5%, ONT: 2%).</li> <li><code>MIN_DEPTH</code> - Sites with read depth below this threshold will be masked. </li> <li><code>MIN_Q</code> - Alleles with a Phred score below this threshold in the reads will be masked.</li> <li><code>MIN_PROP</code> - Minimum Proportion of haplotypes (Wastewater Samples: 0.5%, Clinical Samples: 5%).</li> <li><code>MIN_LEN</code> - Minimum read length to be considered after ivar trim (Deafult: 80).</li> <li><code>MAX_READS</code> - Maximum number of reads considered by WEPP from the sample. Helpful for reducing runtime.</li> <li><code>CLADE_LIST</code> - List the clade annotation schemes used in the MAT. SARS-CoV-2 MAT uses both nextstrain and pango lineage naming systems, so use \"nextstrain,pango\" for it. </li> <li><code>CLADE_IDX</code> - Index used for assigning clades to selected haplotypes from MAT. Use '1' for Pango naming and '0' for Nextstrain naming for SARS-CoV-2. Other pathogens usually follow a single lineage annotation system, so work with '0'. In case of NO lineage annotations, use '-1'. Lineage Annotations could be checked by running: \"matUtils summary -i {TREE} -C {FILENAME}\" -&gt; Use '0' for annotation_1 and '1' for annotation_2. </li> <li><code>DASHBOARD_ENABLED</code> - Set to <code>True</code> to enable the interactive dashboard for viewing WEPP results, or <code>False</code> to disable it.</li> <li><code>TAXONIUM_FILE</code> [Optional] - Name of the user-provided Taxonium <code>.jsonl</code> file for visualization. If specified, this file will be used instead of generating a new one from the given MAT. Ensure that the provided Taxonium file corresponds to the same MAT used for WEPP.</li> </ol>"},{"location":"usage.html#run-command","title":"Run Command","text":"<p>WEPP's snakemake workflow requires <code>DIR</code> and <code>FILE_PREFIX</code> as config arguments through the command line, while the remaining ones can be taken from the config file. It also requires <code>--cores</code> from the command line, which specifies the number of threads used by the workflow.</p> <p>Examples:</p> <ol> <li> <p>Using all the parameters from the config file. <pre><code>snakemake --config DIR=SARS_COV_2_real FILE_PREFIX=test_run TREE=sars_cov_2_mat.pb.gz REF=sars_cov_2_reference.fa --cores 32 --use-conda\n</code></pre></p> </li> <li> <p>Overriding MIN_Q and CLADE_IDX through command line. <pre><code>snakemake --config DIR=SARS_COV_2_real FILE_PREFIX=test_run TREE=sars_cov_2_mat.pb.gz REF=sars_cov_2_reference.fa MIN_Q=25 CLADE_IDX=1 --cores 32 --use-conda\n</code></pre></p> </li> <li> <p>To visualize results from a previous WEPP analysis that was run without the dashboard, set <code>DASHBOARD_ENABLED</code> to <code>True</code> and re-run only the dashboard components, without reanalyzing the dataset. <pre><code>snakemake --config DIR=SARS_COV_2_real FILE_PREFIX=test_run TREE=sars_cov_2_mat.pb.gz REF=sars_cov_2_reference.fa MIN_Q=25 CLADE_IDX=1 DASHBOARD_ENABLED=True --cores 32 --use-conda --forcerun dashboard_serve\n</code></pre></p> </li> </ol> <p>Note</p> <p>\u26a0\ufe0f Use the same configuration parameters (DIR, FILE_PREFIX, etc.) as were used for the specific project. This ensures the dashboard serves the correct results for your chosen dataset.  \u26a0\ufe0f Make sure port forwarding is enabled when running on external servers to view results on your personal machine.</p>"},{"location":"usage.html#mat-download","title":"MAT Download","text":"<p>Mutation-annotated trees (MAT) for different pathogens are maintained by the UShER team, which can be found here. You can also create your own MAT for any pathogen from the consensus genome assemblies using viral_usher.</p>"}]}